{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.25.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import sys\n",
    "#sys.path.append('/scratch/anaconda3/lib/python3.6/site-packages')\n",
    "#sys.path.append('/scratch/michael_git/indic_nlp_library/src')\n",
    "import spacy\n",
    "import torch\n",
    "from torchtext.data import Field, BucketIterator, TabularDataset\n",
    "import torchtext.data as data\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import ToktokTokenizer\n",
    "import glob\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import sentencepiece as spm\n",
    "from araNorm import araNorm\n",
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_BPE_model(bi, mono, lang, vocab_size=20000, savedir='./bpe_models/', add_params=''):\n",
    "    if not isinstance(bi, list):\n",
    "        bi = [bi]\n",
    "    if not isinstance(mono, list):\n",
    "        mono = [mono]\n",
    "    \n",
    "    files = ','.join(bi +  mono)\n",
    "    print(files)\n",
    "    m = savedir + lang #no idea if this will work....\n",
    "    v  = vocab_size\n",
    "    inputs = '--input={} --model_prefix={} --vocab_size={} --model_type=bpe'.format(files,m, v)\n",
    "    inputs = inputs + add_params\n",
    "    print('starting to train ')\n",
    "    spm.SentencePieceTrainer.Train(inputs) #you have to look at terminal to see output\n",
    "    print('finished training, trying to load')\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.Load(m + '.model')\n",
    "    return sp\n",
    "\n",
    "def convertToBPE(model, file, savefile):\n",
    "    to_save = open(savefile, modje='w', encoding='utf-8')\n",
    "    with open(file, mode='r', encoding='utf-8') as f:\n",
    "        print(\"Processing {}\".format(file))\n",
    "        for line in f:\n",
    "            line = model.EncodeAsPieces(line.strip())\n",
    "            to_save.write(\" \".join(line) + '\\n')\n",
    "    to_save.close()\n",
    "    \n",
    "def convertFilesToBPE(model, files):\n",
    "    for f in files:\n",
    "        name = f.split('/')[-1]\n",
    "        pth = f.split(name)[0]\n",
    "        convertToBPE(model, f, pth + 'bpe-' + name )\n",
    "\n",
    "def loadBPEModel(m):\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.Load(m + '.model')\n",
    "    return sp\n",
    "\n",
    "def collect_stats(values):\n",
    "    return np.mean(values), np.std(values), scipy.stats.mode(values)[0], max(values), min(values)\n",
    "\n",
    "def CollectStatistics(pth, model):\n",
    "    sent_lens = [] #defined as white spaces\n",
    "    bpe_lens = [] \n",
    "    \n",
    "    with open(pth, mode='r', encoding='utf-8') as file:\n",
    "        i = 0\n",
    "        for l in file:\n",
    "            l = l.strip()\n",
    "            l_split = l.split()\n",
    "            bpe_split =model.EncodeAsPieces(l)\n",
    "            sent_lens.append(len(l_split))\n",
    "            bpe_lens.append(len(bpe_split))\n",
    "            if i < 10:\n",
    "                print(l_split)\n",
    "                print(bpe_split)\n",
    "            i += 1\n",
    "    print(\"count: {}\".format(i))                  \n",
    "    sent_lens = np.array(sent_lens)\n",
    "    mean, std, mode, max, min = collect_stats(sent_lens)\n",
    "    s = \"mean: {},std: {}, mode: {}, max: {}, min: {}\".format(mean, std, mode, max, min)\n",
    "    print(\"sentence stats: \" + s)\n",
    "    cap_tok =60\n",
    "    print(\"Number of sentences  <= {} tokens: {}\".format(cap_tok, np.sum(sent_lens <= cap_tok)))\n",
    "    bpe_lens = np.array(bpe_lens)\n",
    "    mean, std, mode, max, min = collect_stats(bpe_lens)\n",
    "    s = \"mean: {},std: {}, mode: {}, max: {}, min: {}\".format(mean, std, mode, max, min)\n",
    "    print(\"bpe stats: \" + s)\n",
    "    print(\"Number of bpe  <= {} tokens: {}\".format(cap_tok, np.sum(bpe_lens <= cap_tok)))\n",
    "    \n",
    "    return sent_lens, bpe_lens\n",
    "\n",
    "def removeDiacritics(file, directory='./bpe_models/'):\n",
    "    #this is written for a specific file setup...\n",
    "    normalizer = araNorm()\n",
    "    new_file = open(directory + 'no-diacritics' + file, mode='w', encoding='utf-8')\n",
    "    with open(directory + file, mode='r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = normalizer.run(line)\n",
    "            new_file.write(line + '\\n')\n",
    "    new_file.close()\n",
    "    print('done')\n",
    "\n",
    "\n",
    "bpe_path = './bpe_models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../.data/iwslt/de-en/train.de-en.de\n",
      "starting to train \n",
      "finished training, trying to load\n",
      "../.data/iwslt/de-en/train.de-en.en\n",
      "starting to train \n",
      "finished training, trying to load\n"
     ]
    }
   ],
   "source": [
    "#De -> En\n",
    "#Separate because training the bpe model takes time\n",
    "#Ar -> En\n",
    "\n",
    "b_pth = '../.data/iwslt/de-en/train.de-en.{}'\n",
    "m_pth = '../.data/iwslt/de-en/train.{}'\n",
    "\n",
    "# German\n",
    "b_files = [b_pth.format('de')]\n",
    "m_files = [] #no arabic\n",
    "German = train_BPE_model(b_files, m_files, 'german', vocab_size=10000, savedir='../.data/bpe_models/')\n",
    "\n",
    "# English\n",
    "b_files = [b_pth.format('en') ]\n",
    "m_files = []\n",
    "de_English = train_BPE_model(b_files, m_files, 'de_english', vocab_size=10000, savedir='../.data/bpe_models/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "German\n",
      "['David', 'Gallo:', 'Das', 'ist', 'Bill', 'Lange.', 'Ich', 'bin', 'Dave', 'Gallo.']\n",
      "['▁David', '▁Gall', 'o', ':', '▁Das', '▁ist', '▁Bill', '▁Lange', '.', '▁Ich', '▁bin', '▁Da', 've', '▁Gall', 'o', '.']\n",
      "['Wir', 'werden', 'Ihnen', 'einige', 'Geschichten', 'über', 'das', 'Meer', 'in', 'Videoform', 'erzählen.']\n",
      "['▁Wir', '▁werden', '▁Ihnen', '▁einige', '▁Geschichten', '▁über', '▁das', '▁Meer', '▁in', '▁Video', 'form', '▁erzählen', '.']\n",
      "['Wir', 'haben', 'ein', 'paar', 'der', 'unglaublichsten', 'Aufnahmen', 'der', 'Titanic,', 'die', 'man', 'je', 'gesehen', 'hat,,', 'und', 'wir', 'werden', 'Ihnen', 'nichts', 'davon', 'zeigen.']\n",
      "['▁Wir', '▁haben', '▁ein', '▁paar', '▁der', '▁unglaublich', 'sten', '▁Aufnahmen', '▁der', '▁Titan', 'ic', ',', '▁die', '▁man', '▁je', '▁gesehen', '▁hat', ',', ',', '▁und', '▁wir', '▁werden', '▁Ihnen', '▁nichts', '▁davon', '▁zeigen', '.']\n",
      "['Die', 'Wahrheit', 'ist,', 'dass', 'die', 'Titanic', '–', 'obwohl', 'sie', 'alle', 'Kinokassenrekorde', 'bricht', '–', 'nicht', 'gerade', 'die', 'aufregendste', 'Geschichte', 'vom', 'Meer', 'ist.']\n",
      "['▁Die', '▁Wahrheit', '▁ist', ',', '▁dass', '▁die', '▁Titan', 'ic', '▁–', '▁obwohl', '▁sie', '▁alle', '▁Kin', 'ok', 'assen', 're', 'kor', 'de', '▁bricht', '▁–', '▁nicht', '▁gerade', '▁die', '▁aufregend', 'ste', '▁Geschichte', '▁vom', '▁Meer', '▁ist', '.']\n",
      "['Ich', 'denke,', 'das', 'Problem', 'ist,', 'dass', 'wir', 'das', 'Meer', 'für', 'zu', 'selbstverständlich', 'halten.']\n",
      "['▁Ich', '▁denke', ',', '▁das', '▁Problem', '▁ist', ',', '▁dass', '▁wir', '▁das', '▁Meer', '▁für', '▁zu', '▁selbst', 'verständlich', '▁halten', '.']\n",
      "['Wenn', 'man', 'darüber', 'nachdenkt,', 'machen', 'die', 'Ozeane', '75', '%', 'des', 'Planeten', 'aus.']\n",
      "['▁Wenn', '▁man', '▁darüber', '▁nachdenkt', ',', '▁machen', '▁die', '▁Ozeane', '▁75', '▁', '%', '▁des', '▁Planeten', '▁aus', '.']\n",
      "['Der', 'Großteil', 'der', 'Erde', 'ist', 'Meerwasser.']\n",
      "['▁Der', '▁Großteil', '▁der', '▁Erde', '▁ist', '▁Meerwasser', '.']\n",
      "['Die', 'durchschnittliche', 'Tiefe', 'ist', 'etwa', '3', 'Kilometer.']\n",
      "['▁Die', '▁durchschnitt', 'liche', '▁Tiefe', '▁ist', '▁etwa', '▁3', '▁Kilometer', '.']\n",
      "['Ein', 'Teil', 'des', 'Problems', 'ist,', 'dass', 'wir', 'am', 'Strand', 'stehen', 'oder', 'Bilder', 'wie', 'dieses', 'hier', 'sehen', 'und', 'auf', 'die', 'riesige', 'blaue', 'Weite', 'schauen,', 'und', 'sie', 'schimmert', 'und', 'bewegt', 'sich,', 'es', 'gibt', 'Wellen,', 'Brandung', 'und', 'Gezeiten,', 'aber', 'wir', 'haben', 'keine', 'Ahnung,', 'was', 'darin', 'verborgen', 'ist.']\n",
      "['▁Ein', '▁Teil', '▁des', '▁Problems', '▁ist', ',', '▁dass', '▁wir', '▁am', '▁Strand', '▁stehen', '▁oder', '▁Bilder', '▁wie', '▁dieses', '▁hier', '▁sehen', '▁und', '▁auf', '▁die', '▁riesige', '▁blaue', '▁Wei', 'te', '▁schauen', ',', '▁und', '▁sie', '▁sch', 'immer', 't', '▁und', '▁bewegt', '▁sich', ',', '▁es', '▁gibt', '▁Wellen', ',', '▁Brand', 'ung', '▁und', '▁Ge', 'zeiten', ',', '▁aber', '▁wir', '▁haben', '▁keine', '▁Ahnung', ',', '▁was', '▁darin', '▁verborgen', '▁ist', '.']\n",
      "['In', 'den', 'Ozeanen', 'befinden', 'sich', 'die', 'längsten', 'Gebirgszüge', 'des', 'Planeten.']\n",
      "['▁In', '▁den', '▁Ozeanen', '▁befinden', '▁sich', '▁die', '▁läng', 'sten', '▁Geb', 'ir', 'gs', 'züge', '▁des', '▁Planeten', '.']\n",
      "count: 196884\n",
      "sentence stats: mean: 15.899118262530221,std: 12.204605680803798, mode: [7], max: 661, min: 1\n",
      "Number of sentences  <= 60 tokens: 195644\n",
      "bpe stats: mean: 22.082657808658904,std: 16.879454528605617, mode: [12], max: 820, min: 1\n",
      "Number of bpe  <= 60 tokens: 191562\n",
      "English\n",
      "['David', 'Gallo:', 'This', 'is', 'Bill', 'Lange.', \"I'm\", 'Dave', 'Gallo.']\n",
      "['▁David', '▁Gall', 'o', ':', '▁This', '▁is', '▁Bill', '▁L', 'ange', '.', '▁I', \"'\", 'm', '▁Dave', '▁Gall', 'o', '.']\n",
      "['And', \"we're\", 'going', 'to', 'tell', 'you', 'some', 'stories', 'from', 'the', 'sea', 'here', 'in', 'video.']\n",
      "['▁And', '▁we', \"'\", 're', '▁going', '▁to', '▁tell', '▁you', '▁some', '▁stories', '▁from', '▁the', '▁sea', '▁here', '▁in', '▁video', '.']\n",
      "[\"We've\", 'got', 'some', 'of', 'the', 'most', 'incredible', 'video', 'of', 'Titanic', \"that's\", 'ever', 'been', 'seen,', 'and', \"we're\", 'not', 'going', 'to', 'show', 'you', 'any', 'of', 'it.']\n",
      "['▁We', \"'\", 've', '▁got', '▁some', '▁of', '▁the', '▁most', '▁incredible', '▁video', '▁of', '▁Titanic', '▁that', \"'\", 's', '▁ever', '▁been', '▁seen', ',', '▁and', '▁we', \"'\", 're', '▁not', '▁going', '▁to', '▁show', '▁you', '▁any', '▁of', '▁it', '.']\n",
      "['The', 'truth', 'of', 'the', 'matter', 'is', 'that', 'the', 'Titanic', '--', 'even', 'though', \"it's\", 'breaking', 'all', 'sorts', 'of', 'box', 'office', 'records', '--', \"it's\", 'not', 'the', 'most', 'exciting', 'story', 'from', 'the', 'sea.']\n",
      "['▁The', '▁truth', '▁of', '▁the', '▁matter', '▁is', '▁that', '▁the', '▁Titanic', '▁--', '▁even', '▁though', '▁it', \"'\", 's', '▁breaking', '▁all', '▁sorts', '▁of', '▁box', '▁office', '▁records', '▁--', '▁it', \"'\", 's', '▁not', '▁the', '▁most', '▁exciting', '▁story', '▁from', '▁the', '▁sea', '.']\n",
      "['And', 'the', 'problem,', 'I', 'think,', 'is', 'that', 'we', 'take', 'the', 'ocean', 'for', 'granted.']\n",
      "['▁And', '▁the', '▁problem', ',', '▁I', '▁think', ',', '▁is', '▁that', '▁we', '▁take', '▁the', '▁ocean', '▁for', '▁granted', '.']\n",
      "['When', 'you', 'think', 'about', 'it,', 'the', 'oceans', 'are', '75', 'percent', 'of', 'the', 'planet.']\n",
      "['▁When', '▁you', '▁think', '▁about', '▁it', ',', '▁the', '▁oceans', '▁are', '▁75', '▁percent', '▁of', '▁the', '▁planet', '.']\n",
      "['Most', 'of', 'the', 'planet', 'is', 'ocean', 'water.']\n",
      "['▁Most', '▁of', '▁the', '▁planet', '▁is', '▁ocean', '▁water', '.']\n",
      "['The', 'average', 'depth', 'is', 'about', 'two', 'miles.']\n",
      "['▁The', '▁average', '▁depth', '▁is', '▁about', '▁two', '▁miles', '.']\n",
      "['Part', 'of', 'the', 'problem,', 'I', 'think,', 'is', 'we', 'stand', 'at', 'the', 'beach,', 'or', 'we', 'see', 'images', 'like', 'this', 'of', 'the', 'ocean,', 'and', 'you', 'look', 'out', 'at', 'this', 'great', 'big', 'blue', 'expanse,', 'and', \"it's\", 'shimmering', 'and', \"it's\", 'moving', 'and', \"there's\", 'waves', 'and', \"there's\", 'surf', 'and', \"there's\", 'tides,', 'but', 'you', 'have', 'no', 'idea', 'for', 'what', 'lies', 'in', 'there.']\n",
      "['▁Part', '▁of', '▁the', '▁problem', ',', '▁I', '▁think', ',', '▁is', '▁we', '▁stand', '▁at', '▁the', '▁beach', ',', '▁or', '▁we', '▁see', '▁images', '▁like', '▁this', '▁of', '▁the', '▁ocean', ',', '▁and', '▁you', '▁look', '▁out', '▁at', '▁this', '▁great', '▁big', '▁blue', '▁exp', 'an', 'se', ',', '▁and', '▁it', \"'\", 's', '▁sh', 'im', 'mer', 'ing', '▁and', '▁it', \"'\", 's', '▁moving', '▁and', '▁there', \"'\", 's', '▁waves', '▁and', '▁there', \"'\", 's', '▁surf', '▁and', '▁there', \"'\", 's', '▁t', 'ides', ',', '▁but', '▁you', '▁have', '▁no', '▁idea', '▁for', '▁what', '▁lies', '▁in', '▁there', '.']\n",
      "['And', 'in', 'the', 'oceans,', 'there', 'are', 'the', 'longest', 'mountain', 'ranges', 'on', 'the', 'planet.']\n",
      "['▁And', '▁in', '▁the', '▁oceans', ',', '▁there', '▁are', '▁the', '▁longest', '▁mountain', '▁r', 'anges', '▁on', '▁the', '▁planet', '.']\n",
      "count: 196884\n",
      "sentence stats: mean: 17.12235123219764,std: 13.086066934289212, mode: [8], max: 624, min: 1\n",
      "Number of sentences  <= 60 tokens: 195082\n",
      "bpe stats: mean: 22.044838585156743,std: 16.402018757515375, mode: [11], max: 816, min: 1\n",
      "Number of bpe  <= 60 tokens: 192112\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 9, 14, 24, ..., 31, 11,  4]), array([17, 17, 32, ..., 42, 15,  5]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "b_pth = '../.data/iwslt/de-en/train.de-en.{}'\n",
    "m_pth = '../.data/iwslt/de-en/train.{}'\n",
    "\n",
    "print('German')\n",
    "CollectStatistics(b_pth.format('de'), German)\n",
    "print('English')\n",
    "CollectStatistics(b_pth.format('en'), de_English)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "#convert the arabic file to be w/o diatrics\n",
    "b_pth = '../.data/iwslt/ar-en/'\n",
    "\n",
    "removeDiacritics('/train.ar-en.ar', b_pth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../.data/iwslt/ar-en/no-diacritics/train.ar-en.ar\n",
      "starting to train \n",
      "finished training, trying to load\n",
      "../.data/iwslt/ar-en/train.ar-en.en\n",
      "starting to train \n",
      "finished training, trying to load\n"
     ]
    }
   ],
   "source": [
    "#Ar -> En\n",
    "\n",
    "b_pth = '../.data/iwslt/ar-en/no-diacritics/train.ar-en.{}'\n",
    "#m_pth = './.data/iwslt/ar-en/train.{}'\n",
    "\n",
    "# Arabic\n",
    "b_files = [b_pth.format('ar')]\n",
    "m_files = [] #no arabic\n",
    "Arabic = train_BPE_model(b_files, m_files, 'arabic', vocab_size=10000, savedir='../.data/bpe_models/')\n",
    "\n",
    "\n",
    "b_pth = '../.data/iwslt/ar-en/train.ar-en.{}'\n",
    "# English\n",
    "b_files = [b_pth.format('en') ]\n",
    "m_files = []\n",
    "ar_English = train_BPE_model(b_files, m_files, 'ar_english', vocab_size=10000, savedir='../.data/bpe_models/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arabic\n",
      "['ديفيد', 'جالو:', 'هذا', 'بيل', 'لينج.', 'وأنا', 'ديفيد', 'جالو.']\n",
      "['▁ديفيد', '▁جال', 'و', ':', '▁هذا', '▁بيل', '▁لين', 'ج', '.', '▁و', 'أ', 'نا', '▁ديفيد', '▁جال', 'و', '.']\n",
      "['وسنقوم', 'بإخباركم', 'ببعض', 'القصص', 'من', 'البحر', 'هُنا', 'في', 'الفيديو.']\n",
      "['▁وسن', 'قوم', '▁ب', 'إ', 'خ', 'بار', 'كم', '▁ببعض', '▁القصص', '▁من', '▁البحر', '▁ه', 'ُ', 'نا', '▁في', '▁الفيديو', '.']\n",
      "['لدينا', 'بعض', 'مقاطع', 'فيديو', 'تيتانيك', 'التي', 'لا', 'تصدق', 'ولم', 'يرها', 'أحد', 'إطلاقاً,', 'ونحن', 'لن', 'نستعرض', 'لكم', 'أي', 'منها']\n",
      "['▁لدينا', '▁بعض', '▁مقا', 'طع', '▁فيديو', '▁ت', 'يت', 'اني', 'ك', '▁التي', '▁لا', '▁تصدق', '▁ولم', '▁ير', 'ها', '▁', 'أ', 'حد', '▁', 'إ', 'طلا', 'قا', 'ً', ',', '▁ونحن', '▁لن', '▁نست', 'عرض', '▁لكم', '▁', 'أ', 'ي', '▁منها']\n",
      "['وحقيقة', 'الأمر', 'هو', 'أن', 'تيتانيك', '--', 'رغم', 'انه', 'كسر', 'كل', 'أنواع', 'سجلات', 'شباك', 'التذاكر', '--', 'لكنه', 'ليس', 'أكثر', 'إثارة', 'من', 'قصص', 'البحر.']\n",
      "['▁وح', 'قيق', 'ة', '▁ال', 'أ', 'مر', '▁هو', '▁', 'أ', 'ن', '▁ت', 'يت', 'اني', 'ك', '▁--', '▁رغم', '▁انه', '▁كسر', '▁كل', '▁', 'أ', 'ن', 'واع', '▁س', 'جلات', '▁ش', 'باك', '▁الت', 'ذا', 'كر', '▁--', '▁لكنه', '▁ليس', '▁', 'أ', 'كثر', '▁', 'إ', 'ثار', 'ة', '▁من', '▁قصص', '▁البحر', '.']\n",
      "['المشكلة،', 'أعتقد،', 'هي', 'أننا', 'نعتبر', 'المحيط', 'كشئ', 'مضمون', '.']\n",
      "['▁المشكل', 'ة', '،', '▁', 'أ', 'عتقد', '،', '▁هي', '▁', 'أ', 'ن', 'نا', '▁نعتبر', '▁المحيط', '▁ك', 'شئ', '▁مض', 'مون', '▁.']\n",
      "['وعندما', 'تفكر', 'في', 'ذلك،', 'المحيطات', 'تمثل', 'نسبة', '75%', 'من', 'الأرض.']\n",
      "['▁وعندما', '▁تفكر', '▁في', '▁ذلك', '،', '▁المحيطات', '▁تمثل', '▁نسب', 'ة', '▁75', '%', '▁من', '▁ال', 'أ', 'رض', '.']\n",
      "['معظم', 'مياة', 'كواكب', 'الأرض', 'محيطات', '.']\n",
      "['▁معظم', '▁م', 'يا', 'ة', '▁كواكب', '▁ال', 'أ', 'رض', '▁محيط', 'ات', '▁.']\n",
      "['متوسط', 'عمقها', 'حوالي', 'ميلين.']\n",
      "['▁متوسط', '▁عمق', 'ها', '▁حوالي', '▁مي', 'لين', '.']\n",
      "['جزء', 'من', 'المشكلة،', 'أعتقد،', 'أننا', 'نقف', 'على', 'الشاطئ', 'أو', 'نرى', 'صور', 'مثل', 'هذه', 'للمحيط،', 'وأنت', 'تنظر', 'لهذه', 'الفسحة', 'الكبيرة', 'جداً', 'الزرقاءالمتلألئة', 'وهي', 'تتحرك', 'ولديها', 'موجات', 'ولديها', 'مد', 'وجزر،', 'لكن', 'ليس', 'لديك', 'فكرة', 'عن', 'ما', 'يكمن', 'في', 'وجودها.']\n",
      "['▁جزء', '▁من', '▁المشكل', 'ة', '،', '▁', 'أ', 'عتقد', '،', '▁', 'أ', 'ن', 'نا', '▁ن', 'قف', '▁عل', 'ى', '▁الشاطئ', '▁', 'أ', 'و', '▁نر', 'ى', '▁صور', '▁مثل', '▁هذه', '▁للم', 'حيط', '،', '▁و', 'أ', 'نت', '▁تنظر', '▁لهذه', '▁الف', 'سح', 'ة', '▁الكبير', 'ة', '▁جدا', 'ً', '▁الزرقاء', 'الم', 'تل', 'أ', 'ل', 'ئ', 'ة', '▁وهي', '▁تتحرك', '▁ولد', 'يها', '▁موجات', '▁ولد', 'يها', '▁مد', '▁وج', 'زر', '،', '▁لكن', '▁ليس', '▁لديك', '▁فكر', 'ة', '▁عن', '▁ما', '▁يكمن', '▁في', '▁وجود', 'ها', '.']\n",
      "['وفي', 'المحيطات', 'هنالك', 'أطول', 'جبل', 'على', 'سطح', 'الأرض.']\n",
      "['▁وفي', '▁المحيطات', '▁هنالك', '▁', 'أ', 'ط', 'ول', '▁جبل', '▁عل', 'ى', '▁سطح', '▁ال', 'أ', 'رض', '.']\n",
      "count: 225002\n",
      "sentence stats: mean: 14.319330494840045,std: 11.096016074009087, mode: [7], max: 559, min: 1\n",
      "Number of sentences  <= 60 tokens: 224055\n",
      "bpe stats: mean: 27.876707762597665,std: 21.887273079688875, mode: [12], max: 1112, min: 1\n",
      "Number of bpe  <= 60 tokens: 209370\n",
      "English\n",
      "['David', 'Gallo:', 'This', 'is', 'Bill', 'Lange.', \"I'm\", 'Dave', 'Gallo.']\n",
      "['▁David', '▁G', 'all', 'o', ':', '▁This', '▁is', '▁Bill', '▁L', 'ange', '.', '▁I', \"'\", 'm', '▁D', 'ave', '▁G', 'all', 'o', '.']\n",
      "['And', \"we're\", 'going', 'to', 'tell', 'you', 'some', 'stories', 'from', 'the', 'sea', 'here', 'in', 'video.']\n",
      "['▁And', '▁we', \"'\", 're', '▁going', '▁to', '▁tell', '▁you', '▁some', '▁stories', '▁from', '▁the', '▁sea', '▁here', '▁in', '▁video', '.']\n",
      "[\"We've\", 'got', 'some', 'of', 'the', 'most', 'incredible', 'video', 'of', 'Titanic', \"that's\", 'ever', 'been', 'seen,', 'and', \"we're\", 'not', 'going', 'to', 'show', 'you', 'any', 'of', 'it.']\n",
      "['▁We', \"'\", 've', '▁got', '▁some', '▁of', '▁the', '▁most', '▁incredible', '▁video', '▁of', '▁Titan', 'ic', '▁that', \"'\", 's', '▁ever', '▁been', '▁seen', ',', '▁and', '▁we', \"'\", 're', '▁not', '▁going', '▁to', '▁show', '▁you', '▁any', '▁of', '▁it', '.']\n",
      "['The', 'truth', 'of', 'the', 'matter', 'is', 'that', 'the', 'Titanic', '--', 'even', 'though', \"it's\", 'breaking', 'all', 'sorts', 'of', 'box', 'office', 'records', '--', \"it's\", 'not', 'the', 'most', 'exciting', 'story', 'from', 'the', 'sea.']\n",
      "['▁The', '▁truth', '▁of', '▁the', '▁matter', '▁is', '▁that', '▁the', '▁Titan', 'ic', '▁--', '▁even', '▁though', '▁it', \"'\", 's', '▁breaking', '▁all', '▁sorts', '▁of', '▁box', '▁office', '▁records', '▁--', '▁it', \"'\", 's', '▁not', '▁the', '▁most', '▁exciting', '▁story', '▁from', '▁the', '▁sea', '.']\n",
      "['And', 'the', 'problem,', 'I', 'think,', 'is', 'that', 'we', 'take', 'the', 'ocean', 'for', 'granted.']\n",
      "['▁And', '▁the', '▁problem', ',', '▁I', '▁think', ',', '▁is', '▁that', '▁we', '▁take', '▁the', '▁ocean', '▁for', '▁granted', '.']\n",
      "['When', 'you', 'think', 'about', 'it,', 'the', 'oceans', 'are', '75', 'percent', 'of', 'the', 'planet.']\n",
      "['▁When', '▁you', '▁think', '▁about', '▁it', ',', '▁the', '▁oceans', '▁are', '▁75', '▁percent', '▁of', '▁the', '▁planet', '.']\n",
      "['Most', 'of', 'the', 'planet', 'is', 'ocean', 'water.']\n",
      "['▁Most', '▁of', '▁the', '▁planet', '▁is', '▁ocean', '▁water', '.']\n",
      "['The', 'average', 'depth', 'is', 'about', 'two', 'miles.']\n",
      "['▁The', '▁average', '▁depth', '▁is', '▁about', '▁two', '▁miles', '.']\n",
      "['Part', 'of', 'the', 'problem,', 'I', 'think,', 'is', 'we', 'stand', 'at', 'the', 'beach,', 'or', 'we', 'see', 'images', 'like', 'this', 'of', 'the', 'ocean,', 'and', 'you', 'look', 'out', 'at', 'this', 'great', 'big', 'blue', 'expanse,', 'and', \"it's\", 'shimmering', 'and', \"it's\", 'moving', 'and', \"there's\", 'waves', 'and', \"there's\", 'surf', 'and', \"there's\", 'tides,', 'but', 'you', 'have', 'no', 'idea', 'for', 'what', 'lies', 'in', 'there.']\n",
      "['▁Part', '▁of', '▁the', '▁problem', ',', '▁I', '▁think', ',', '▁is', '▁we', '▁stand', '▁at', '▁the', '▁beach', ',', '▁or', '▁we', '▁see', '▁images', '▁like', '▁this', '▁of', '▁the', '▁ocean', ',', '▁and', '▁you', '▁look', '▁out', '▁at', '▁this', '▁great', '▁big', '▁blue', '▁exp', 'an', 'se', ',', '▁and', '▁it', \"'\", 's', '▁sh', 'im', 'mer', 'ing', '▁and', '▁it', \"'\", 's', '▁moving', '▁and', '▁there', \"'\", 's', '▁waves', '▁and', '▁there', \"'\", 's', '▁surf', '▁and', '▁there', \"'\", 's', '▁t', 'ides', ',', '▁but', '▁you', '▁have', '▁no', '▁idea', '▁for', '▁what', '▁lies', '▁in', '▁there', '.']\n",
      "['And', 'in', 'the', 'oceans,', 'there', 'are', 'the', 'longest', 'mountain', 'ranges', 'on', 'the', 'planet.']\n",
      "['▁And', '▁in', '▁the', '▁oceans', ',', '▁there', '▁are', '▁the', '▁long', 'est', '▁mountain', '▁r', 'anges', '▁on', '▁the', '▁planet', '.']\n",
      "count: 225002\n",
      "sentence stats: mean: 17.330663727433535,std: 13.424254367453015, mode: [8], max: 624, min: 1\n",
      "Number of sentences  <= 60 tokens: 222658\n",
      "bpe stats: mean: 23.05692393845388,std: 17.40869473604644, mode: [12], max: 878, min: 1\n",
      "Number of bpe  <= 60 tokens: 218005\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 9, 14, 24, ...,  9,  2,  1]), array([20, 17, 33, ..., 10,  7,  4]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_pth = '../.data/iwslt/ar-en/train.ar-en.{}'\n",
    "m_pth = '../.data/iwslt/ar-en/train.{}'\n",
    "\n",
    "print('Arabic')\n",
    "CollectStatistics(b_pth.format('ar'), Arabic)\n",
    "print('English')\n",
    "CollectStatistics(b_pth.format('en'), ar_English)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# because... torch text doesn't combine validation data nicely you have to do it your self -,-\n",
    "def write_from_file_to_other(filepth, target_file):\n",
    "    with open(filepth, 'r', encoding='utf-8') as file:\n",
    "        for l in file:\n",
    "            if len(l.strip()) == 0:\n",
    "                continue\n",
    "            else:\n",
    "                target_file.write(l.strip() + '\\n')\n",
    "def merge_iwslt_bitext(pth, src, trg, write_pth):\n",
    "    #get the files\n",
    "    entries = [p for p in glob.glob(pth) if '.xml' not in p]\n",
    "    entries = set([e[:-3] for e in entries])\n",
    "    \n",
    "    src_file = open(write_pth + '.' + src, 'w', encoding='utf-8')\n",
    "    trg_file = open(write_pth + '.' + trg, 'w', encoding='utf-8')\n",
    "    for e in entries:\n",
    "        print(e + '.' + src)\n",
    "        write_from_file_to_other(e + '.' + src, src_file)\n",
    "        write_from_file_to_other(e + '.' + trg, trg_file)\n",
    "                \n",
    "    src_file.close()\n",
    "    trg_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../.data/iwslt/de-en/IWSLT16.TEDX.dev2012.de-en.de\n",
      "../.data/iwslt/de-en/IWSLT16.TED.dev2010.de-en.de\n"
     ]
    }
   ],
   "source": [
    "merge_iwslt_bitext('../.data/iwslt/de-en/IWSLT16.TED*.dev*.de-en.*', 'de', 'en', '../.data/iwslt/de-en/val.de-en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../.data/iwslt/de-en/IWSLT16.TED.tst2014.de-en.de\n",
      "../.data/iwslt/de-en/IWSLT16.TED.tst2012.de-en.de\n",
      "../.data/iwslt/de-en/IWSLT16.TED.tst2011.de-en.de\n",
      "../.data/iwslt/de-en/IWSLT16.TEDX.tst2013.de-en.de\n",
      "../.data/iwslt/de-en/IWSLT16.TED.tst2010.de-en.de\n",
      "../.data/iwslt/de-en/IWSLT16.TEDX.tst2014.de-en.de\n",
      "../.data/iwslt/de-en/IWSLT16.TED.tst2013.de-en.de\n"
     ]
    }
   ],
   "source": [
    "merge_iwslt_bitext('../.data/iwslt/de-en/IWSLT16.TED*.tst*.de-en.*', 'de', 'en', '../.data/iwslt/de-en/test.de-en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../.data/iwslt/en-de/IWSLT16.TED.dev2010.en-de.en\n"
     ]
    }
   ],
   "source": [
    "merge_iwslt_bitext('../.data/iwslt/en-de/IWSLT16.TED*.dev*.en-de.*', \\\n",
    "                   src='en', trg='de', write_pth='../.data/iwslt/en-de/val.en-de')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../.data/iwslt/en-de/IWSLT16.TED.tst2014.en-de.en\n",
      "../.data/iwslt/en-de/IWSLT16.TED.tst2010.en-de.en\n",
      "../.data/iwslt/en-de/IWSLT16.TED.tst2011.en-de.en\n",
      "../.data/iwslt/en-de/IWSLT16.TED.tst2012.en-de.en\n",
      "../.data/iwslt/en-de/IWSLT16.TED.tst2013.en-de.en\n"
     ]
    }
   ],
   "source": [
    "merge_iwslt_bitext('../.data/iwslt/en-de/IWSLT16.TED*.tst*.en-de.*', \\\n",
    "                   src='en', trg='de', write_pth='../.data/iwslt/en-de/test.en-de')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../.data/iwslt/en-ar/IWSLT16.TED.dev2010.en-ar.en\n",
      "../.data/iwslt/en-ar/IWSLT16.TED.tst2010.en-ar.en\n",
      "../.data/iwslt/en-ar/IWSLT16.TED.tst2012.en-ar.en\n",
      "../.data/iwslt/en-ar/IWSLT16.TED.tst2011.en-ar.en\n",
      "../.data/iwslt/en-ar/IWSLT16.TED.tst2013.en-ar.en\n",
      "../.data/iwslt/en-ar/IWSLT16.TED.tst2014.en-ar.en\n"
     ]
    }
   ],
   "source": [
    "merge_iwslt_bitext('../.data/iwslt/en-ar/IWSLT16.TED*.dev*.en-ar.*', \\\n",
    "                   src='en', trg='ar', write_pth='../.data/iwslt/en-ar/val.en-ar')\n",
    "\n",
    "merge_iwslt_bitext('../.data/iwslt/en-ar/IWSLT16.TED*.tst*.en-ar.*', \\\n",
    "                   src='en', trg='ar', write_pth='../.data/iwslt/en-ar/test.en-ar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../.data/iwslt/ar-en/IWSLT16.TED.dev2010.ar-en.ar\n",
      "../.data/iwslt/ar-en/IWSLT16.TED.tst2012.ar-en.ar\n",
      "../.data/iwslt/ar-en/IWSLT16.TED.tst2013.ar-en.ar\n",
      "../.data/iwslt/ar-en/IWSLT16.TED.tst2014.ar-en.ar\n",
      "../.data/iwslt/ar-en/IWSLT16.TED.tst2010.ar-en.ar\n",
      "../.data/iwslt/ar-en/IWSLT16.TED.tst2011.ar-en.ar\n"
     ]
    }
   ],
   "source": [
    "merge_iwslt_bitext('../.data/iwslt/ar-en/IWSLT16.TED*.dev*.ar-en.*', \\\n",
    "                   src='ar', trg='en', write_pth='../.data/iwslt/ar-en/val.ar-en')\n",
    "\n",
    "merge_iwslt_bitext('../.data/iwslt/ar-en/IWSLT16.TED*.tst*.ar-en.*', \\\n",
    "                   src='ar', trg='en', write_pth='../.data/iwslt/ar-en/test.ar-en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
