{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/scratch/anaconda3/lib/python3.6/site-packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import torch\n",
    "from torchtext.data import Field, BucketIterator, TabularDataset\n",
    "import torchtext.data as data\n",
    "from gensim.utils import tokenize\n",
    "import pandas as pd\n",
    "import googletrans\n",
    "import glob\n",
    "from xml.etree.ElementTree import parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just some debugging stuff to check tmx\n",
    "tree = parse('./mono_hi-ne/Nepali_biblecorpus.xml')\n",
    "root = tree.getroot()[1] #This gets the body of the tmx file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "{'id': 'Bible', 'lang': 'np'}\n",
      "{'id': 'b.GEN', 'type': 'book'}\n",
      "{'id': 'b.GEN.1', 'type': 'chapter'}\n",
      "आरम्भमा परमेश्वरले आकाश र पृथ्वी सृष्टि गर्नु भयो।\n"
     ]
    }
   ],
   "source": [
    "print(len(root))\n",
    "print(root[0].attrib) # body of the xml file\n",
    "print(root[0][0].attrib) #Book\n",
    "print(root[0][0][0].attrib) #chapter\n",
    "print(root[0][0][0][0].text.strip()) #actual verse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def bibleXMLtoTxt(path):\n",
    "    tree = parse(path)\n",
    "    body = tree.getroot()[1][0] #should get you body of da tree\n",
    "    sentences = []\n",
    "    for book in body:\n",
    "        for chapter in book:\n",
    "            for seg in chapter:\n",
    "                text = seg.text\n",
    "                if text is None:\n",
    "                    continue\n",
    "                sentences.append(text.strip())\n",
    "    return sentences\n",
    "\n",
    "def TMXtoTxt(path, L1, L2):\n",
    "    tree = parse(path) #you can parse TMX as an xml file\n",
    "    root = tree.getroot()[1] #should get body of tmx (where pairs are)\n",
    "    \n",
    "    data = {L1: [None] * len(root), L2: [None] * len(root)}\n",
    "    kept = 0\n",
    "    for i , child in enumerate(root):\n",
    "        l1 = list(child[0].attrib.keys())[0]\n",
    "        l1 = child[0].attrib[l1]\n",
    "        l2 = list(child[1].attrib.keys())[0]\n",
    "        l2 = child[1].attrib[l2]\n",
    "        \n",
    "        assert l1 == L1 and l2 == L2, \"seems there is a misalignment of language {} != {} or {} != {}\".format(l1, L1, l2, L2)\n",
    "        if len(child[0][0].text) > 0 and len(child[1][0].text) > 0:\n",
    "            data[l1][i] = child[0][0].text\n",
    "            data[l2][i] = child[1][0].text\n",
    "            kept += 1\n",
    "    print(\"number kept {} / {}\".format(kept, i))\n",
    "    \n",
    "    assert len(data[l1]) == len(data[l2]), \"You don't have equal pairs\"\n",
    "    #something like this\n",
    "    return data    \n",
    "\n",
    "def checkTSV(path):\n",
    "    with open(path, encoding='utf-8') as f:\n",
    "        i = 0\n",
    "        for l in f:\n",
    "            pair = l.strip().split('\\t')\n",
    "            if len(pair) > 2 or len(pair) == 1:\n",
    "                print('\\n')\n",
    "                print(l.strip())\n",
    "                print(i, pair)\n",
    "            i += 1\n",
    "    print(i)\n",
    "    \n",
    "def recoverEntry(entries):\n",
    "    #here are the 2 patterns i've seen europarl (do not use this as a general solution)\n",
    "    if len(entries) == 3 and len(entries[1]) == 0: #this means we had 2 entries in between each entry\n",
    "        return [entries[0], entries[-1]]\n",
    "    elif len(entries) == 3 and (entries[0].strip() in entries[2]): #i have seen this...once, very skeptical if this will work\n",
    "        return [entries[0] + entries[1], entries[2]]\n",
    "    elif len(entries) == 3 and (entries[0].strip() not in entries[2]):\n",
    "        return [entries[1], entries[2]] #again 1 bug that happened in es-pt\n",
    "    elif len(entries) == 4 and entries[0] == entries[2]: #Weird split from numbers\n",
    "        return [entries[0] + entries[1], entries[2] + entries[3]]\n",
    "    else:\n",
    "        print('failed to recover:')\n",
    "        print(entries)\n",
    "        return []\n",
    "    \n",
    "def cleanTSV(path, l1, l2, thresh=3):\n",
    "    results = {l1:[], l2: []}\n",
    "    with open(path, encoding='utf-8') as f:\n",
    "        i = 0\n",
    "        for l in f:\n",
    "            i += 1\n",
    "            entries = l.strip().split('\\t')\n",
    "            if len(entries) > 2:\n",
    "                entries = recoverEntry(entries)\n",
    "            if len(entries) <= 1:\n",
    "                continue #there's no pair so just ignore it\n",
    "            non_empty = len(entries[0]) > 0 and len(entries[1]) > 0\n",
    "            long_enough = len(entries[0].strip()) > thresh and len(entries[0].strip()) > thresh \n",
    "            if len(entries) == 2 and non_empty and long_enough :\n",
    "                results[l1].append(entries[0])\n",
    "                results[l2].append(entries[1])\n",
    "    assert len(results[l1]) == len(results[l2]), \"You don't have an equal number of sentences\"\n",
    "    print('Entries kept {} / {}'.format(len(results[l1]), i))\n",
    "    return results\n",
    "\n",
    "def merge_all_data(dicts, l1, l2):\n",
    "    all_data = {l1: [], l2: []}\n",
    "    for d in dicts:\n",
    "        all_data[l1] =  all_data[l1] + d[l1].copy()\n",
    "        all_data[l2] =  all_data[l2] + d[l2].copy()\n",
    "        assert len(all_data[l1]) == len(all_data[l2]), \"ugh oh, unaligned bitext\"\n",
    "    return all_data       \n",
    "\n",
    "def lookup_words(x, vocab=None):\n",
    "    if vocab is not None:\n",
    "        x = [vocab.itos[i] for i in x]\n",
    "    return [str(t) for t in x]\n",
    "\n",
    "def buildVocab(sentences, tokenizer,min_freq=1):\n",
    "    vocab = {}\n",
    "    total_tokens = 0\n",
    "    for sent in sentences:\n",
    "        for t in tokenizer(sent):\n",
    "            total_tokens += 1\n",
    "            if t in vocab:\n",
    "                vocab[t] += 1\n",
    "            else:\n",
    "                vocab[t] = 1\n",
    "\n",
    "    vocab = [k for k in vocab.keys() if vocab[k] > min_freq]\n",
    "    print('Vocab size {} with min_freq {}'.format(len(vocab), min_freq))\n",
    "    print('Total tokens counted {}'.format(total_tokens))\n",
    "    return vocab\n",
    "\n",
    "def writeSentenceList(sentences, path):\n",
    "    \n",
    "    with open(path, encoding='utf-8', mode='w') as f:\n",
    "        for s in sentences:\n",
    "            f.write(s + '\\n')    \n",
    "\n",
    "def merge_all(pth, dir, l):\n",
    "    files = glob.glob(pth)\n",
    "    out = open(dir + 'all.' + l, mode='w', encoding='utf-8')\n",
    "    for f in files:\n",
    "        with open(f, mode='r', encoding='utf-8') as file:\n",
    "            for l in file:\n",
    "                out.write(l)\n",
    "    out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30547\n",
      "31065\n"
     ]
    }
   ],
   "source": [
    "# Hindi - Nepali Cleaning\n",
    "pth = './mono_hi-ne/'\n",
    "\n",
    "nepali_bible = bibleXMLtoTxt(pth + 'Nepali_biblecorpus.xml')\n",
    "print(len(nepali_bible))\n",
    "writeSentenceList(nepali_bible, pth + 'bible.ne')\n",
    "hindi_bible = bibleXMLtoTxt( pth + 'Hindi_biblecorpus.xml')\n",
    "writeSentenceList(hindi_bible, pth + 'bible.hi')\n",
    "print(len(hindi_bible))\n",
    "with open(pth + 'hindmonocorp05.plaintext', mode='r', encoding='utf-8') as src, open(pth + 'monocorp05.hi', mode='w', encoding='utf-8') as trg :\n",
    "    for line in src:\n",
    "        line = line.split('>')[-1]\n",
    "        line = line.strip()\n",
    "        trg.write(line + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hindi - Nepali Merge\n",
    "pth = './mono_hi-ne/'\n",
    "merge_all(pth + '*.ne*', pth, 'ne')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "europarl-v9.cs cs\n",
      "news.2018.pl.shuffled.deduped cs\n",
      "europarl-v9.pl cs\n",
      "news-commentary-v14.cs cs\n"
     ]
    }
   ],
   "source": [
    "#Czech - polish translation data\n",
    "pth='./mono_cs-pl/'\n",
    "\n",
    "files = glob.glob(pth + '*.*')\n",
    "for f in files:\n",
    "    name = f.split('/')[-1]\n",
    "    print(name, ext)\n",
    "    with open(f, mode='r', encoding='utf-8') as f:\n",
    "        out = open(pth + 'clean-' + name, mode='w', encoding='utf-8')\n",
    "        for l in f:\n",
    "            l = l.strip()\n",
    "            if len(l) > 4:\n",
    "                out.write(l + '\\n')\n",
    "        out.close()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Czech - polish translation data merge monolingual data together\n",
    "pth='./mono_cs-pl/'\n",
    "merge_all(pth + 'clean-*cs*', pth, 'cs')\n",
    "merge_all(pth + 'clean-*pl*', pth, 'pl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./mono_es-pt/news.2012.es.shuffled.deduped', './mono_es-pt/news.2016.pt.shuffled.deduped', './mono_es-pt/news.2010.es.shuffled.deduped', './mono_es-pt/news.2011.pt.shuffled.deduped', './mono_es-pt/news.2018.pt.shuffled.deduped', './mono_es-pt/news-commentary-v14.es', './mono_es-pt/news.2011.es.shuffled.deduped', './mono_es-pt/news.2012.pt.shuffled.deduped', './mono_es-pt/news.2008.es.shuffled.deduped', './mono_es-pt/news.2017.pt.shuffled.deduped', './mono_es-pt/news.2014.pt.shuffled.deduped', './mono_es-pt/news.2009.pt.shuffled.deduped', './mono_es-pt/news.2014.es.shuffled.deduped', './mono_es-pt/news.2015.es.shuffled.deduped', './mono_es-pt/news.2007.es.shuffled.deduped', './mono_es-pt/europarl-v9.es', './mono_es-pt/news.2013.pt.shuffled.deduped', './mono_es-pt/news.2016.es.shuffled.deduped', './mono_es-pt/news.2009.es.shuffled.deduped', './mono_es-pt/news.2008.pt.shuffled.deduped', './mono_es-pt/news-commentary-v14.pt', './mono_es-pt/news.2018.es.shuffled.deduped', './mono_es-pt/europarl-v9.pt', './mono_es-pt/news.2015.pt.shuffled.deduped', './mono_es-pt/news.2010.pt.shuffled.deduped', './mono_es-pt/news.2013.es.shuffled.deduped', './mono_es-pt/news.2017.es.shuffled.deduped']\n",
      "news.2012.es.shuffled.deduped\n",
      "news.2016.pt.shuffled.deduped\n",
      "news.2010.es.shuffled.deduped\n",
      "news.2011.pt.shuffled.deduped\n",
      "news.2018.pt.shuffled.deduped\n",
      "news-commentary-v14.es\n",
      "news.2011.es.shuffled.deduped\n",
      "news.2012.pt.shuffled.deduped\n",
      "news.2008.es.shuffled.deduped\n",
      "news.2017.pt.shuffled.deduped\n",
      "news.2014.pt.shuffled.deduped\n",
      "news.2009.pt.shuffled.deduped\n",
      "news.2014.es.shuffled.deduped\n",
      "news.2015.es.shuffled.deduped\n",
      "news.2007.es.shuffled.deduped\n",
      "europarl-v9.es\n",
      "news.2013.pt.shuffled.deduped\n",
      "news.2016.es.shuffled.deduped\n",
      "news.2009.es.shuffled.deduped\n",
      "news.2008.pt.shuffled.deduped\n",
      "news-commentary-v14.pt\n",
      "news.2018.es.shuffled.deduped\n",
      "europarl-v9.pt\n",
      "news.2015.pt.shuffled.deduped\n",
      "news.2010.pt.shuffled.deduped\n",
      "news.2013.es.shuffled.deduped\n",
      "news.2017.es.shuffled.deduped\n"
     ]
    }
   ],
   "source": [
    "#Spanih Portugese translation\n",
    "pth = './mono_es-pt/'\n",
    "\n",
    "files = glob.glob(pth + '*.*')\n",
    "files = [f for f in files if '_crawl.sh' not in f]\n",
    "for f in files:\n",
    "    name = f.split('/')[-1]\n",
    "    print(name)\n",
    "    with open(f, mode='r', encoding='utf-8') as f:\n",
    "        out = open(pth + 'clean-' + name, mode='w', encoding='utf-8')\n",
    "        for l in f:\n",
    "            l = l.strip()\n",
    "            if len(l) > 4:\n",
    "                out.write(l + '\\n')\n",
    "        out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spanih Portugese merge\n",
    "pth = './mono_es-pt/'\n",
    "merge_all(pth + 'clean-*.es*', pth, 'es')\n",
    "merge_all(pth + 'clean-*.pt*', pth, 'pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65506\n",
      "3001\n"
     ]
    }
   ],
   "source": [
    "#Create the Hindi Nepali dataset\n",
    "#slightly different from all the other ones but not by much \n",
    "def getDataFromFile(pth, name, l1, l2):\n",
    "    l1_l2_dict = {l1:[], l2: []}\n",
    "    with open(pth + name + '.' + l1, encoding='utf-8') as f:\n",
    "        l1_l2_dict[l1] = list(f.read().split('\\n'))\n",
    "    with open(pth + name + '.' + l2, encoding='utf-8') as f:\n",
    "        l1_l2_dict[l2] = list(f.read().split('\\n'))\n",
    "    assert len(l1_l2_dict[l1]) == len(l1_l2_dict[l2]), \"ugh oh, unaligned\"\n",
    "    print(len(l1_l2_dict[l1]))\n",
    "    return l1_l2_dict\n",
    "pth = './hi-ne/TrainDevSimilar/'\n",
    "\n",
    "#train set\n",
    "he_ne_train = getDataFromFile(pth, 'train', 'hi', 'ne')\n",
    "pd.DataFrame.from_dict(he_ne_train).to_csv(pth + 'clean-train-hi-ne.tsv',sep='\\t',index=False)\n",
    "#dev set \n",
    "he_ne_dev = getDataFromFile(pth, 'dev2019', 'hi', 'ne')\n",
    "pd.DataFrame.from_dict(he_ne_dev).to_csv(pth + 'clean-dev-hi-ne.tsv',sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
